{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52e63368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training labels: [3 0 1 1 2]\n",
      "Test labels: [0]\n",
      "Test labels: ['I am worried that he felt safe', 'I am not happy that she is worried', 'She is happy', 'He is not worried', 'He is safe']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# Here is some example tweets and labels for both training and test\n",
    "# Each tweet has one label that corresponds to {0: 'anger', 1: 'joy', 2: 'optimism', 3: 'sadness'}\n",
    "train_data = ['I am worried that he felt safe', \n",
    "              'I am not happy that she is worried', \n",
    "              'She is happy', \n",
    "              'He is not worried',\n",
    "              'He is safe']\n",
    "\n",
    "test_data = ['I do not like being worried']\n",
    "\n",
    "train_labels = np.array([3,0,1,1,2])\n",
    "test_labels = np.array([0])\n",
    "\n",
    "print(f'Training labels: {train_labels}')\n",
    "print(f'Test labels: {test_labels}')\n",
    "\n",
    "print(f'Test labels: {train_data}')\n",
    "\n",
    "# Semantics of this label vectors:\n",
    "# We have 4 unique labels [0,1,2,3]\n",
    "# These two vectors consists of a label for each sentence in training (5) and test (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "008b1377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training features: \n",
      "[[1 1 0 1 0 0 1 0 1 1]\n",
      " [1 0 1 0 1 1 0 1 1 1]\n",
      " [0 0 1 0 1 0 0 1 0 0]\n",
      " [0 0 0 1 1 1 0 0 0 1]\n",
      " [0 0 0 1 1 0 1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# To obtain features (in 2d numpy array) of this training data you can use scikit-learn CountVectorizer\n",
    "# Here an example:\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "counter = CountVectorizer()\n",
    "# assign this as self.counter in the assignment code\n",
    "\n",
    "# You can use counter.fit_transform() to count words for training data \n",
    "train_feats = counter.fit_transform(train_data).toarray()\n",
    "print(f'Training features: \\n{train_feats}')\n",
    "\n",
    "# This array (dimension 2x10) shows individual word counts for each tweet, therefore 1 vector per tweet\n",
    "\n",
    "# For Task 1 in the Assignment, you can use CountVectorizer for the test data as well\n",
    "# Please be careful that you need to use ***the same counter*** for both training and test data\n",
    "# Here, you can use counter.transform() for already created counter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9c95f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test features: \n",
      "[[0 0 0 0 0 1 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "# This is the feature array for test data that we are supposed to get with get_features\n",
    "test_feats = np.array([[0, 0, 0, 0, 0, 1, 0, 0, 0, 1]])\n",
    "\n",
    "print(f'Test features: \\n{test_feats}')\n",
    "\n",
    "# Semantics of this features array:\n",
    "# We have 1 tweet in test data \n",
    "# Each number shows the count of unique words in the training tweets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "472eae67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tweets: 5\n",
      "Number of tweets per class: \n",
      "[1 2 1 1]\n"
     ]
    }
   ],
   "source": [
    "# For Naive Bayes implementation:\n",
    "# First we need to calculate class priors from training data: \n",
    "# class priors = number of tweets for a class / total number of tweets\n",
    "\n",
    "# number of all tweets in training data\n",
    "no_of_training_sentences = train_feats.shape[0]\n",
    "print(f'Total number of tweets: {no_of_training_sentences}')\n",
    "\n",
    "# number of tweets per class\n",
    "no_tweets_by_class = np.bincount(train_labels)\n",
    "print(f'Number of tweets per class: \\n{no_tweets_by_class}')\n",
    "\n",
    "# You need to divide the number of tweet per class by the total number of tweets to get priors !!!\n",
    "# Save this as self.priors in the assignment code\n",
    "# Shape of priors should be (4,): one probability value for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2062c75e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique class labels: \n",
      "[0 1 2 3]\n",
      "Training features per class: \n",
      "[[array([1, 0, 1, 0, 1, 1, 0, 1, 1, 1], dtype=int64)], [array([0, 0, 1, 0, 1, 0, 0, 1, 0, 0], dtype=int64), array([0, 0, 0, 1, 1, 1, 0, 0, 0, 1], dtype=int64)], [array([0, 0, 0, 1, 1, 0, 1, 0, 0, 0], dtype=int64)], [array([1, 1, 0, 1, 0, 0, 1, 0, 1, 1], dtype=int64)]]\n",
      "Word counts per class: \n",
      "[array([1, 0, 1, 0, 1, 1, 0, 1, 1, 1], dtype=int64), array([0, 0, 1, 1, 2, 1, 0, 1, 0, 1], dtype=int64), array([0, 0, 0, 1, 1, 0, 1, 0, 0, 0], dtype=int64), array([1, 1, 0, 1, 0, 0, 1, 0, 1, 1], dtype=int64)]\n"
     ]
    }
   ],
   "source": [
    "# Next step for naive bayes is to calculate word likelihoods per class:\n",
    "# Word likelihoods =  Number of words per class / number of total word count per class \n",
    "\n",
    "# We need to split training features by class\n",
    "# first lets obtain our label set (unique labels from training labels)\n",
    "unique_labels = np.unique(train_labels)\n",
    "print(f'Unique class labels: \\n{unique_labels}')\n",
    "\n",
    "# Let's create a dictionary for each unique label with empty list \n",
    "features_by_label = [[] for c in unique_labels]\n",
    "\n",
    "# Now we need to append all the feature vectors belong to corresponding label\n",
    "for tweet, label in zip(train_feats, train_labels):\n",
    "    features_by_label[label].append(tweet)   \n",
    "print(f'Training features per class: \\n{features_by_label}')\n",
    "\n",
    "# Next step, accumulating the word counts per label \n",
    "feature_sum_by_label = [[] for c in unique_labels]\n",
    "for label in unique_labels:\n",
    "    feature_sum_by_label[label] = np.sum(features_by_label[label], axis=0)\n",
    "print(f'Word counts per class: \\n{feature_sum_by_label}')\n",
    "    \n",
    "# you also need to add 1 more count to each word to avoid 0 count (alpha smoothing)\n",
    "\n",
    "# Later, you need to divide word counts by total number of words for each class !!!\n",
    "# Save this as self.word_likelihoods in the assignment code\n",
    "\n",
    "# Shape of resulting word_likelihoods should be (4, 10) in this example: one probability value for each word, in each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15778424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word likelihood per each class: \n",
      " [array([0.45975667, 0.4624187 , 0.73438367, 0.27105114, 0.33603853,\n",
      "       0.12564712, 0.86881108, 0.65509601, 0.9937313 , 0.09298962]), array([0.09347038, 0.46538082, 0.13814393, 0.3324499 , 0.21272205,\n",
      "       0.31374194, 0.39387099, 0.9546204 , 0.51821175, 0.19964157]), array([0.5502145 , 0.62462802, 0.18761569, 0.31628705, 0.94929205,\n",
      "       0.4244766 , 0.71161382, 0.85834696, 0.36983051, 0.84902922]), array([0.31516285, 0.84915602, 0.88114888, 0.55011002, 0.54256614,\n",
      "       0.23719129, 0.13993389, 0.06873989, 0.50092118, 0.61709189])]\n",
      "word indexes exist (non-zero count) in your test tweet: \n",
      " [5 9]\n",
      "Total likelihood probabilities of this test tweet: \n",
      " [0.01168387721270875, 0.06263593364827179, 0.3603930382497615, 0.14636882043044644]\n"
     ]
    }
   ],
   "source": [
    "# Let's create a random word likelihood array to simulate our code:\n",
    "word_likelihoods = [np.random.rand(10) for l in unique_labels]\n",
    "print(f'Word likelihood per each class: \\n {word_likelihoods}')\n",
    "\n",
    "# After you obtain class priors and word likelihoods, your training is done !!!\n",
    "# Next step is to predict test labels using already calculated class priors and word likelihoods\n",
    "\n",
    "# Here, we need to loop over each test tweets (In this example we only have 1 test tweet):\n",
    "for i, test_tweet in enumerate(test_feats):\n",
    "    \n",
    "    # Some word may not be in your test data\n",
    "    # by using numpy you can easily obtain word indexes exist (non-zero count) in your test tweet\n",
    "    word_exists_idx = np.flatnonzero(test_tweet)\n",
    "    print(f'word indexes exist (non-zero count) in your test tweet: \\n {word_exists_idx}')\n",
    "    \n",
    "    # after obtain this word indexes, let's obtain tweet likelihood per class:\n",
    "    likelihoods_of_tweet_by_label = [1 for l in unique_labels]\n",
    "    for label in unique_labels:\n",
    "            \n",
    "        # Here, we can loop test feature to calculate likelihood of the tweet\n",
    "        for idx in word_exists_idx:\n",
    "            likelihoods_of_tweet_by_label[label] *= (word_likelihoods[label][idx] ** test_tweet[idx])\n",
    "            # Second term is for words with count more than one \n",
    "\n",
    "    print(f'Total likelihood probabilities of this test tweet: \\n {likelihoods_of_tweet_by_label}')\n",
    "    \n",
    "    # Here, you need to multiply this resulting tweet probabilities with the class prior for each class to obtain conditional probabilities\n",
    "    # As the final step you need to select the class with maximum conditional probability\n",
    "    # You can use np.argmax function for this operation !!!\n",
    "    \n",
    "    # Do it for each test tweet and obtain predicted classes !!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}